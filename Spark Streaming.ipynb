{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importación y creación del contexto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importación de dependencias y funciones\n",
    "from __future__ import print_function\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "from operator import add\n",
    "from operator import sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load external packages programatically\n",
    "import os\n",
    "packages = \"org.apache.spark:spark-streaming-kafka-0-8_2.11:2.2.1\"\n",
    "# os.environ[\"PYSPARK_PYTHON\"] = \"\"\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = (\n",
    "    \"--packages {0} pyspark-shell\".format(packages)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc = SparkContext(appName=\"streaming\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Crear el contexto de Spark Streaming\n",
    "ssc = StreamingContext(sc, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parseo de datos de entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "def parseOrder(line):\n",
    "    \n",
    "    s = re.split(''',(?=(?:[^'\"]|'[^']*'|\"[^\"]*\")*$)''', line)\n",
    "    \n",
    "    try:\n",
    "       \n",
    "      return [{\"ID\": s[0],\n",
    "               \"PARENT-SYS-ID\": s[1],\n",
    "               \"Source\": s[2],\n",
    "               \"Mentions\":s[3],\n",
    "               \"Target\":s[4],\n",
    "               \"Name_source\":s[5],\n",
    "               \"Body\": s[6],\n",
    "               \"Pub_date\":s[7],\n",
    "               \"URLs\":s[8],\n",
    "               \"Tipe_action\":s[9],\n",
    "               \"Link\":s[10],\n",
    "               \"Has_link\":s[11],\n",
    "               \"Has_picture\":s[12],\n",
    "               \"Website\":s[13],\n",
    "               \"Country\":s[14],\n",
    "               \"Activity\":s[15],\n",
    "               \"Followers\":s[16],\n",
    "               \"Following\":s[17],\n",
    "                \"Location\":s[18]\n",
    "               }]\n",
    "    except Exception as err:\n",
    "        print(\"Wrong line format (%s): \" % line)\n",
    "        return []\n",
    "    \n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Envio de datos a Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Configura el endpoint para localizar el broker de Kafka\n",
    "# kafkaBrokerIPPort = \"172.20.1.21:9092\"\n",
    "kafkaBrokerIPPort = \"127.0.0.1:9092\"\n",
    "\n",
    "# Productor simple (Singleton!)\n",
    "# from kafka import KafkaProducer\n",
    "import kafka\n",
    "class KafkaProducerWrapper(object):\n",
    "  producer = None\n",
    "  @staticmethod\n",
    "  def getProducer(brokerList):\n",
    "    if KafkaProducerWrapper.producer != None:\n",
    "      return KafkaProducerWrapper.producer\n",
    "    else:\n",
    "      KafkaProducerWrapper.producer = kafka.KafkaProducer(bootstrap_servers=brokerList,\n",
    "                                                          key_serializer=str.encode,\n",
    "                                                          value_serializer=str.encode)\n",
    "      return KafkaProducerWrapper.producer\n",
    "\n",
    "# Envía métricas a Kafka! (salida)  \n",
    "def sendMetrics(itr):\n",
    "  prod = KafkaProducerWrapper.getProducer([kafkaBrokerIPPort])\n",
    "  for m in itr:\n",
    "    prod.send(\"metrics\", key=m[0], value=m[0]+\",\"+str(m[1]))\n",
    "  prod.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lectura de datos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fichero de texto: Lectura de fuente de datos de fichero (no se usa en este ejemplo, en su lugar \n",
    "# enviamos los datos a Kafka para crear una simulación más realista)\n",
    "#stream = ssc.textFileStream(\"data/input\")\n",
    "\n",
    "# Kafka: Lectura de datos\n",
    "kafkaParams = {\"metadata.broker.list\": kafkaBrokerIPPort}\n",
    "stream = KafkaUtils.createDirectStream(ssc, [\"Quatar_GP_2014\"], kafkaParams)\n",
    "stream = stream.map(lambda o: str(o[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcular el número total de menciones recibidas por cada cuenta de usuario durante el intervalo de 5 segundos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "orders = stream.flatMap(parseOrder)\n",
    "#Filtramos aquellos mensajes que sean menciones\n",
    "orders_filter = orders.filter((lambda x: True if 'M' in x['Tipe_action'] else False ))\n",
    "#Un vez tenemos la información filtrada eliminamos aquellos que tengan el campo vacio.\n",
    "orders_filter_not = orders_filter.filter((lambda x: True if x['Tipe_action'] != '' else False ))\n",
    "#Split y explode para separar cada uno por la coma.\n",
    "\n",
    "orders_alone = (orders_filter_not.filter((lambda x: x['Mentions']))\n",
    "                                 .flatMap(lambda x: x['Mentions'].split(\",\"))\n",
    "                                 .map(lambda word: (word, 1)) \n",
    "                                 .reduceByKey(lambda a, b: a + b))\n",
    "\n",
    "\n",
    "orders_alone.foreachRDD(lambda rdd: rdd.foreachPartition(sendMetrics))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcular la frecuencia total acumulada de apariciones de cada hastag en el campo body, actulizando un ranking con los 5 hastags con mayor frecuencia de aparicion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "orders = stream.flatMap(parseOrder)\n",
    "\n",
    "#Cálculo de frecuencias.\n",
    "frecuencias = (orders.filter((lambda x: x['Body']))\n",
    "                                 .flatMap(lambda o: o['Body'].split(\" \"))\n",
    "                                 .filter(lambda word: True if len(word) != 0 else False)\n",
    "                                 .filter(lambda word: True if word[0] == '#' else False)\n",
    "                                 .map(lambda word: (word, 1)) \n",
    "                                 .reduceByKey(lambda a, b: a + b))\n",
    "\n",
    "#Top 5 hastags\n",
    "FrecuencyState = frecuencias.updateStateByKey(lambda vals, totalOpt: sum(vals) + totalOpt if totalOpt != None else sum(vals))\n",
    "\n",
    "top5 = (FrecuencyState.transform(lambda rdd: rdd.sortBy(lambda x: x[1], False)\n",
    "                   .map(lambda x: x[0]).zipWithIndex().filter(lambda x: x[1] < 5)))\n",
    "\n",
    "top5values = (FrecuencyState.transform(lambda rdd: rdd.sortBy(lambda x: x[1], False)\n",
    "                   .map(lambda x: x[1]).zipWithIndex().filter(lambda x: x[1] < 5)))\n",
    "\n",
    "top5List = top5values.repartition(1).map(lambda x: str(x[0])).glom().map(lambda arr: (\"Top5values\", arr))\n",
    "\n",
    "top5clList = top5.repartition(1).map(lambda x: str(x[0])).glom().map(lambda arr: (\"Top5\", arr))\n",
    "\n",
    "\n",
    "finalStream = FrecuencyState.union(top5clList)\n",
    "\n",
    "finalStream_2 = FrecuencyState.union(top5List)\n",
    "\n",
    "\n",
    "finalStream.foreachRDD(lambda rdd: rdd.foreachPartition(sendMetrics))\n",
    "\n",
    "finalStream_2.foreachRDD(lambda rdd: rdd.foreachPartition(sendMetrics))\n",
    "\n",
    "\n",
    "sc.setCheckpointDir(\"data_entrega/checkpoint/\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcular en una ventana temporal 20 segundos con offset de 10 segundos la frecuencia de aparición de cada uno de los 3 posibles tipos de tweets (TW-RT-MT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clase(m):\n",
    "    \n",
    "    if 'TW' in m:\n",
    "        return('TW')\n",
    "    \n",
    "    elif 'Rt' in m:\n",
    "        \n",
    "        return('RT')\n",
    "    \n",
    "    else:\n",
    "        return('MT')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "orders = stream.flatMap(parseOrder)\n",
    "\n",
    "orders_filter = orders.filter(lambda x: True if x['Tipe_action'] != '' else False ).filter((lambda x: x['Tipe_action']))\n",
    "\n",
    "change = (orders_filter.map((lambda x: clase(x['Tipe_action'])))\n",
    "                              .map(lambda word: (word, 1)) \n",
    "                              .reduceByKeyAndWindow(add, sub, 20, 10))\n",
    "                              \n",
    "    \n",
    "\n",
    "\n",
    "change.foreachRDD(lambda rdd: rdd.foreachPartition(sendMetrics))\n",
    "\n",
    "sc.setCheckpointDir(\"data_entrega/checkpoint/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Streaming context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ssc.start()\n",
    "ssc.awaitTerminationOrTimeout(10)# Espera 10 segs. antes de acabar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop Streaming context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ssc.stop(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
